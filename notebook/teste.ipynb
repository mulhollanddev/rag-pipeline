{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1cf0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe50ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest():\n",
    "    # Get the doc\n",
    "    loader = PyPDFLoader(\"/Users/yehao/Desktop/projetos/wmb/dataset/QuickStatementsBasics.pdf\")\n",
    "    pages = loader.load_and_split()\n",
    "    # Split the pages by char\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split {len(pages)} documents into {len(chunks)} chunks.\")\n",
    "    #\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    #Create vector store\n",
    "    Chroma.from_documents(documents=chunks,  embedding=embedding, persist_directory=\"./sql_chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5f8ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 16 documents into 36 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yehao/Desktop/projetos/wmb/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:16<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# only run this once to generate vector store\n",
    "ingest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "448f4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_read = \"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\"\n",
    "access_token_write = \"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\"\n",
    "login(token = access_token_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7add175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain():\n",
    "    model = ChatOllama(model=\"llama3\")\n",
    "    #\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "            <s> [Instructions] You are an expert assistant in Wikidata tools. \n",
    "            Answer the following question strictly based on the given context, which may contain information about QuickStatements, \n",
    "            SPARQL queries, item creation, property formatting, or submission strategies. \n",
    "            If the context is insufficient to answer, reply with: \"No relevant context available to answer this question.\" [/Instructions] </s> \n",
    "\n",
    "            [Instructions] \n",
    "            Question: {input} \n",
    "            Context: {context} \n",
    "            Answer: \n",
    "            [/Instructions]\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    #Load vector store\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    vector_store = Chroma(persist_directory=\"./sql_chroma_db\", embedding_function=embedding)\n",
    "\n",
    "    #Create chain\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"score_threshold\": 0.5,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    document_chain = create_stuff_documents_chain(model, prompt)\n",
    "    chain = create_retrieval_chain(retriever, document_chain)\n",
    "    #\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cff010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d431af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query: str):\n",
    "    #\n",
    "    chain = rag_chain()\n",
    "    # invoke chain\n",
    "    result = chain.invoke({\"input\": query})\n",
    "    # print results\n",
    "    print(result[\"answer\"])\n",
    "    for doc in result[\"context\"]:\n",
    "        print(\"Source: \", doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08c55847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-02 08:50:44.384\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m430\u001b[0m - \u001b[31m\u001b[1mCould not download model from HuggingFace: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=None)\"), '(Request ID: 1c0e2136-badf-42d6-9edb-f1dd6a30697e)') Falling back to other sources.\u001b[0m\n",
      "\u001b[32m2025-08-02 08:50:44.387\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m452\u001b[0m - \u001b[31m\u001b[1mCould not download model from either source, sleeping for 3.0 seconds, 2 retries left.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuickStatements (QS) is a tool that allows you to edit Wikidata items using a simple set of text commands. With QS, you can add and remove statements, labels, descriptions, aliases, and add statements with optional qualifiers and sources. The command sequence can be typed in the import window or created in a spreadsheet or text editor and pasted in, or even generated by external code like Lua, called from a template and passed as a URL.\n",
      "Source:  /Users/yehao/Desktop/projetos/wmb/dataset/QuickStatementsBasics.pdf\n"
     ]
    }
   ],
   "source": [
    "ask(\"What is quicksatements?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
