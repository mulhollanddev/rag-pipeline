# Rag



## ü¶ô Instala√ß√£o do Ollama com o modelo LLaMA 3 no Windows 

### üì• 1. Baixar e instalar o Ollama

üëâ [https://ollama.com/download](https://ollama.com/download)

1. Fa√ßa o download do instalador `.exe`
2. Execute e conclua a instala√ß√£o normalmente
3. Ou via termnial
>> winget install Ollama.Ollama
---

### ‚öôÔ∏è 2. Adicionar o Ollama ao PATH do Windows
>> [System.Environment]::SetEnvironmentVariable('Path', [System.Environment]::GetEnvironmentVariable('Path', 'Machine') + ';C:\Program Files\Ollama', 'Machine')


### ‚öôÔ∏è 3. Utilizar o modelo

#### Modelo mais pesado
>> ollama pull llama3:8b

#### Modelo mais leve
>> ollama pull llama3:instruct

#### Script de teste
from langchain_ollama import ChatOllama  # e n√£o mais langchain_community

llm = ChatOllama(model="llama3:instruct")
resposta = llm.invoke("Qual √© a capital da Alemanha?")
print(resposta.content)


## ü¶ô Instala√ß√£o do Ollama com o modelo LLaMA 3 no Linux/Mac
### üì• 1. Baixar e instalar o Ollama

üëâ [https://ollama.com/download](https://ollama.com/download)
/ no linux
>> curl -fsSL https://ollama.com/install.sh | sh

## Conectar com o Hugging Face
1. Add o comando no terminal
>> huggingface-cli login

2. Inserir o token do hugging face que pode ser obtido no endere√ßo
[https://huggingface.co/settings/tokens]
