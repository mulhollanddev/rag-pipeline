
# ğŸ§  RAG com LLaMA 3 e Ollama

## âš™ï¸ InstalaÃ§Ã£o de dependÃªncias do projeto

```bash
pip install -r requirements.txt
```

---

## ğŸ¦™ InstalaÃ§Ã£o do Ollama com o modelo **LLaMA 3** no **Windows**

### ğŸ“¥ 1. Baixar e instalar o Ollama

- Acesse: ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)
- Baixe o instalador `.exe` e conclua a instalaÃ§Ã£o.
- **Ou instale via terminal**:

```powershell
winget install Ollama.Ollama
```

---

### ğŸ› ï¸ 2. Adicionar o Ollama ao `PATH` do Windows

Execute no PowerShell (como administrador):

```powershell
[System.Environment]::SetEnvironmentVariable('Path', [System.Environment]::GetEnvironmentVariable('Path', 'Machine') + ';C:\Program Files\Ollama', 'Machine')
```

---

### ğŸ§ª 3. Baixar e testar o modelo

#### ğŸ“¦ Baixar o modelo:

Modelo completo (mais pesado):

```bash
ollama pull llama3:8b
```

Modelo otimizado (mais leve):

```bash
ollama pull llama3:instruct
```

#### âœ… Exemplo de uso com LangChain:

```python
from langchain_ollama import ChatOllama  # Atualizado! NÃ£o use langchain_community

llm = ChatOllama(model="llama3:instruct")
resposta = llm.invoke("Qual Ã© a capital da Alemanha?")
print(resposta.content)
```

---

## ğŸ§ InstalaÃ§Ã£o do Ollama com o modelo **LLaMA 3** no **Linux/macOS**

### ğŸ“¥ 1. Baixar e instalar o Ollama

- Acesse: ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)
- Ou, via terminal (Linux/macOS):

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

---

## ğŸ¤ IntegraÃ§Ã£o com Hugging Face (opcional)

1. FaÃ§a login via terminal:

```bash
huggingface-cli login
```

2. Insira seu token de acesso, obtido em:

ğŸ‘‰ [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

---

## ğŸ› ï¸ Comando rÃ¡pido para baixar o modelo LLaMA 3

```bash
ollama pull llama3:instruct
```
